{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will implement DCGAN on SVHN dataset. The dataset is available at http://ufldl.stanford.edu/housenumbers/. The dataset is a collection of 32x32 color images of house numbers. The dataset is split into 3 parts: train, test and extra. We will use the train and test set for training and testing respectively. The extra set is not used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"c_dcgan7\"\n",
    "#check model saving path is there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms as transformations\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "import torchvision.models as models\n",
    "#numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from ignite.metrics.gan import FID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and constants: for dataset and training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters etc.\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 2e-4  # could also use two lrs, one for gen and one for disc\n",
    "BATCH_SIZE = 6400\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS_IMG = 3\n",
    "NUM_CLASSES = 10\n",
    "GEN_EMBEDDING = 100\n",
    "NOISE_DIM = 100\n",
    "NUM_EPOCHS = 50\n",
    "FEATURES_DISC = 64\n",
    "FEATURES_GEN = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we will use SVHN dataset for this example\n",
    "### we will combine the train, test and extra datasets to make a bigger dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: dataset_svhm/train_32x32.mat\n",
      "Using downloaded and verified file: dataset_svhm/test_32x32.mat\n",
      "Using downloaded and verified file: dataset_svhm/extra_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "#get the dataset\n",
    "#train part of svhn\n",
    "train_dataset = datasets.SVHN(root=\"dataset_svhm/\", split='train', transform=transforms, download=True)\n",
    "#test part of svhn\n",
    "test_dataset = datasets.SVHN(root=\"dataset_svhm/\", split='test', transform=transforms, download=True)\n",
    "#extra part of svhn\n",
    "extra_dataset = datasets.SVHN(root=\"dataset_svhm/\", split='extra', transform=transforms, download=True)\n",
    "#concatenate the train, test and extra dataset\n",
    "dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset, extra_dataset])\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630420\n",
      "torch.Size([3, 64, 64])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#print the total number of images in the dataset\n",
    "print(len(dataset))\n",
    "# print the shape of the images\n",
    "print(dataset[0][0].shape)\n",
    "# print the label of the image\n",
    "print(dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g, num_classes, img_size, embed_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.net = nn.Sequential(\n",
    "           \n",
    "            self.generator_block_architecture(channels_noise+embed_size, features_g * 16, 4, 1, 0),  # img: 4x4\n",
    "            self.generator_block_architecture(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
    "            self.generator_block_architecture(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
    "            self.generator_block_architecture(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "     \n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        self.embed = nn.Embedding(num_classes, embed_size)\n",
    "\n",
    "    def generator_block_architecture(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
    "        x = torch.cat([x, embedding], 1)\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d, num_classes, img_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.disc = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(\n",
    "                channels_img+1, features_d, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self.dicriminator_block_architecture(features_d, features_d * 2, 4, 2, 1),\n",
    "            self.dicriminator_block_architecture(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self.dicriminator_block_architecture(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "       \n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        #embedding for conditionning\n",
    "        self.embed = nn.Embedding(num_classes, img_size * img_size)\n",
    "        \n",
    "\n",
    "    def dicriminator_block_architecture(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        embedding = self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size)\n",
    "        x = torch.cat([x, embedding], dim=1)\n",
    "        return self.disc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization : Model , Loss , Optimizer, data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES, IMAGE_SIZE, GEN_EMBEDDING).to(device)\n",
    "disc = Discriminator(CHANNELS_IMG, FEATURES_DISC, NUM_CLASSES, IMAGE_SIZE).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(disc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 09:42:43.257425: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fixed_noise = torch.randn(100, NOISE_DIM, 1, 1).to(device)\n",
    "#plot loss of generator and critic\n",
    "#fixed labels for tensorboard plotting\n",
    "# we will have fixed labels of integers between 0 and 9 for the 10 classes\n",
    "fixed_labels = torch.randint(0, 10, (100,)).to(device)\n",
    "\n",
    "writer_loss = SummaryWriter(f\"runs/\"+model_name+\"/loss\")\n",
    "writer_real = SummaryWriter(f\"logs/\"+model_name+\"/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/\"+model_name+\"/fake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize FID wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_score = FID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpolate function to resize images to 299,299,3  which is the input size of inception network\n",
    "def interpolate(batch):\n",
    "    arr = []\n",
    "    for img in batch:\n",
    "        pil_img = transformations.ToPILImage()(img)\n",
    "        resized_img = pil_img.resize((299,299), Image.BILINEAR)\n",
    "        arr.append(transformations.ToTensor()(resized_img))\n",
    "    return torch.stack(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (disc): Sequential(\n",
       "    (0): Conv2d(4, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       "  (embed): Embedding(10, 4096)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.train()\n",
    "disc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 0/631                   Loss D: 0.6926, loss G: 0.7361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3950/595671961.py:6: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  resized_img = pil_img.resize((299,299), Image.BILINEAR)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID score:  0.14767076964138187\n",
      "Epoch [0/50] Batch 10/631                   Loss D: 0.4764, loss G: 1.2017\n",
      "FID score:  0.14395326712041215\n",
      "Epoch [0/50] Batch 20/631                   Loss D: 0.2346, loss G: 1.9186\n",
      "FID score:  0.13147245206264982\n",
      "Epoch [0/50] Batch 30/631                   Loss D: 0.1133, loss G: 2.5157\n",
      "FID score:  0.1922484396250087\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    \n",
    "    #we will track the total loss of the generator and critic for each epoch over the entire dataset\n",
    "    #initialize the total loss of the generator and critic for each epoch to 0\n",
    "    total_loss_gen = 0\n",
    "    total_loss_disc = 0\n",
    "    #move these to device\n",
    "    \n",
    "    \n",
    "    # Target labels not needed! <3 unsupervised\n",
    "    for batch_idx, (real,labels ) in enumerate(dataloader):\n",
    "        #send labels to device\n",
    "        labels = labels.to(device)\n",
    "        batch_step = 0\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "        \n",
    "        if len(noise) != len(labels):\n",
    "                noise = noise[:len(labels)]\n",
    "        \n",
    "    \n",
    "        \n",
    "        fake = gen(noise, labels)\n",
    "\n",
    "        ### Train Discriminator\n",
    "        disc_real = disc(real, labels).reshape(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake.detach(), labels).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator:\n",
    "        output = disc(fake, labels).reshape(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            total_loss_gen += loss_gen.item()\n",
    "            total_loss_disc += loss_disc.item()\n",
    "            \n",
    "        \n",
    "\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                               \n",
    "                 #BATCH LOSS---\n",
    "        \n",
    "                #write loss to tensorboard\n",
    "                writer_loss.add_scalar(\"Generator loss Batch\", loss_gen, global_step=batch_step)\n",
    "                writer_loss.add_scalar(\"Discriminator loss Batch\", loss_disc, global_step=batch_step)         \n",
    "                \n",
    "                #FID--\n",
    "                #calculate FID score of this batch\n",
    "                #update the fid_score with real and fake images\n",
    "                real_images_fid = interpolate(real)\n",
    "                fake_images_fid = interpolate(fake)\n",
    "                fid_score.update((real_images_fid, fake_images_fid))\n",
    "                computed_fid_score = fid_score.compute()\n",
    "                print(\"FID score: \", computed_fid_score)\n",
    "                writer_loss.add_scalar(\"FID Score DCGAN\", computed_fid_score, global_step=batch_step)\n",
    "                #reset the fid score\n",
    "                fid_score.reset()\n",
    "                ##FID--\n",
    "                \n",
    "                batch_step += 1 \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake = gen(fixed_noise, fixed_labels)\n",
    "        # take out upto 100 examples\n",
    "        img_grid_real = torchvision.utils.make_grid(\n",
    "            real[:100], normalize=True\n",
    "        )\n",
    "        img_grid_fake = torchvision.utils.make_grid(\n",
    "            fake[:100], normalize=True\n",
    "        )\n",
    "\n",
    "        writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "        writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "        \n",
    "        \n",
    "        #AVERAGE LOSS---\n",
    "\n",
    "        #get average loss of generator and critic for each epoch\n",
    "        avg_loss_gen = total_loss_gen / len(loader)\n",
    "        avg_loss_disc= total_loss_disc / len(loader)\n",
    "        #write loss to tensorboard\n",
    "        writer_loss.add_scalar(\"Generator loss Epoch\", avg_loss_gen, global_step=batch_step)\n",
    "        writer_loss.add_scalar(\"Discriminator loss Epoch\", avg_loss_disc, global_step=batch_step)\n",
    "        \n",
    "        #AVERAGE LOSS----\n",
    "        \n",
    "        #we will plot the gradient of disc output with respect to the input image\n",
    "        #get the gradient of the disc output with respect to the input image\n",
    "        gradient = torch.autograd.grad(\n",
    "        inputs=real,\n",
    "        outputs=disc_real,\n",
    "        grad_outputs=torch.ones_like(disc_real),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        )[0]\n",
    "        #flatten the gradient\n",
    "        gradient = gradient.view(gradient.shape[0], -1)\n",
    "        #get the norm of the gradient\n",
    "        gradient_norm = gradient.norm(2, dim=1)\n",
    "        #write gradient norm to tensorboard\n",
    "        writer_loss.add_scalar(\"Gradient norm Disc Real DCGAN\", gradient_norm.mean(), global_step=step)\n",
    "        \n",
    "        #----------------\n",
    "        #we will plot the gradient of critic output with respect to the input image\n",
    "        #get the gradient of the critic output with respect to the input image\n",
    "        gradient = torch.autograd.grad(\n",
    "        inputs=fake,\n",
    "        outputs=disc_fake,\n",
    "        grad_outputs=torch.ones_like(disc_fake),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        )[0]\n",
    "        #flatten the gradient\n",
    "        gradient = gradient.view(gradient.shape[0], -1)\n",
    "        #get the norm of the gradient\n",
    "        gradient_norm = gradient.norm(2, dim=1)\n",
    "        #write gradient norm to tensorboard\n",
    "        writer_loss.add_scalar(\"Gradient norm Disc Fake DCGAN\", gradient_norm.mean(), global_step=step)\n",
    "        \n",
    "        #----------------\n",
    "        #we will plot the gradient of genrator output with respect to the input \n",
    "        #we will plot the gradient of genrator output with respect to the input \n",
    "        #get the gradient of the generator output with respect to the input noise\n",
    "        gradient = torch.autograd.grad(\n",
    "        inputs=noise,\n",
    "        outputs=output,\n",
    "        grad_outputs=torch.ones_like(output),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        )[0]\n",
    "        #flatten the gradient\n",
    "        gradient = gradient.view(gradient.shape[0], -1)\n",
    "        #get the norm of the gradient\n",
    "        gradient_norm = gradient.norm(2, dim=1)\n",
    "        #write gradient norm to tensorboard\n",
    "        writer_loss.add_scalar(\"Gradient norm Generator DCGAN\", gradient_norm.mean(), global_step=step)\n",
    "        \n",
    "        #----------------\n",
    "        \n",
    "        #get the gradient of the disc for the parameters weights of first layer\n",
    "        #we will write the norm of the gardient of weights of the first layer of the disc\n",
    "        for name, param in critic.named_parameters():\n",
    "            if name == \"disc.0.weight\":\n",
    "                writer_loss.add_scalar(\"Disc Gradient w.r.t 1st layer DCGAN\", param.grad.norm(), global_step=step)\n",
    "            #also plot the norm of gradient of 2nd layer\n",
    "            elif name == \"disc.2.0.weight\":\n",
    "                writer_loss.add_scalar(\"Disc Gradient w.r.t 2nd layer DCGAN\", param.grad.norm(), global_step=step)\n",
    "                \n",
    "                \n",
    "       \n",
    "\n",
    "    step += 1\n",
    "    \n",
    "    #save the trained model\n",
    "        #check if trained_model folder exists\n",
    "    if not os.path.exists(\"trained_models\"):\n",
    "        os.mkdir(\"trained_models\")\n",
    "    \n",
    "    #now trained_model folder exists\n",
    "    if not os.path.exists(\"trained_models/\"+model_name):\n",
    "        os.mkdir(\"trained_models/\"+model_name)\n",
    "    #check if \"trained_models/\"+model_name     \n",
    "    torch.save(gen.state_dict(), \"trained_models/\"+model_name+\"/gen.pth\")\n",
    "    torch.save(critic.state_dict(), \"trained_models/\"+model_name+\"/disc.pth\")\n",
    "    \n",
    "    #save checkpoint\n",
    "    # save seperate checkpoint for generator and critic\n",
    "    #save generator in a file\n",
    "    save_checkpoint(gen, optimizer_gen, filename=\"trained_models/\"+model_name+\"/gen_checkpoint.pth.tar\")\n",
    "    save_checkpoint(critic, optimizer_disc, filename=\"trained_models/\"+model_name+\"/disc_checkpoint.pth.tar\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the tensorboard\n",
    "writer_real.close()\n",
    "writer_fake.close()\n",
    "writer_loss.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
