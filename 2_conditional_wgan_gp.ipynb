{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will train a conditional wgan on svhn dataset\n",
    "# we will use the gradient penalty to stabilize the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"c_wgan7\"\n",
    "#check model saving path is there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms as transformations\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "import torchvision.models as models\n",
    "#numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from ignite.metrics.gan import FID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameters for the dataset and the MOdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters etc.\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 64000\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS_IMG = 3\n",
    "NUM_CLASSES = 10\n",
    "GEN_EMBEDDING = 100\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 50\n",
    "FEATURES_CRITIC = 16\n",
    "FEATURES_GEN = 16\n",
    "CRITIC_ITERATIONS = 5\n",
    "LAMBDA_GP = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we will use SVHN dataset for this example\n",
    "### we will combine the train, test and extra datasets to make a bigger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: dataset_svhm/train_32x32.mat\n",
      "Using downloaded and verified file: dataset_svhm/test_32x32.mat\n",
      "Using downloaded and verified file: dataset_svhm/extra_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "#get the dataset\n",
    "#train part of svhn\n",
    "train_dataset = datasets.SVHN(root=\"dataset_svhm/\", split='train', transform=transforms, download=True)\n",
    "#test part of svhn\n",
    "test_dataset = datasets.SVHN(root=\"dataset_svhm/\", split='test', transform=transforms, download=True)\n",
    "#extra part of svhn\n",
    "extra_dataset = datasets.SVHN(root=\"dataset_svhm/\", split='extra', transform=transforms, download=True)\n",
    "#concatenate the train, test and extra dataset\n",
    "dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset, extra_dataset])\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630420\n",
      "torch.Size([3, 64, 64])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#print the total number of images in the dataset\n",
    "print(len(dataset))\n",
    "# print the shape of the images\n",
    "print(dataset[0][0].shape)\n",
    "# print the label of the image\n",
    "print(dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g , num_classes, img_size, embed_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self.block_architecture_generator(channels_noise+embed_size, features_g * 16, 4, 1, 0),  # img: 4x4\n",
    "            self.block_architecture_generator(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
    "            self.block_architecture_generator(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
    "            self.block_architecture_generator(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            # Output: N x channels_img x 64 x 64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        self.embed = nn.Embedding(num_classes, embed_size)\n",
    "\n",
    "    def block_architecture_generator(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
    "        x = torch.cat([x, embedding], 1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d, num_classes, img_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.disc = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(channels_img+1, features_d, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self.block_architecture_critic(features_d, features_d * 2, 4, 2, 1),\n",
    "            self.block_architecture_critic(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self.block_architecture_critic(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "        )\n",
    "        \n",
    "        #embedding for conditionning\n",
    "        self.embed = nn.Embedding(num_classes, img_size * img_size)\n",
    "\n",
    "    def block_architecture_critic(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        embedding = self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size)\n",
    "        x = torch.cat([x, embedding], dim=1)\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize gen and critic\n",
    "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES, IMAGE_SIZE, GEN_EMBEDDING).to(device)\n",
    "critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC, NUM_CLASSES, IMAGE_SIZE).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializate optimizer\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inintialize tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 19:22:54.885499: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# for tensorboard plotting\n",
    "fixed_noise = torch.randn(100, Z_DIM, 1, 1).to(device)\n",
    "#fixed labels for tensorboard plotting\n",
    "# we will have fixed labels of integers between 0 and 9 for the 10 classes\n",
    "fixed_labels = torch.randint(0, 10, (100,)).to(device)\n",
    "\n",
    "#plot loss of generator and critic\n",
    "writer_loss = SummaryWriter(f\"runs/\"+model_name+\"/loss\")\n",
    "writer_real = SummaryWriter(f\"logs/\"+model_name+\"/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/\"+model_name+\"/fake\")\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize FID wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_score = FID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpolate function to resize images to 299,299,3  which is the input size of inception network\n",
    "def interpolate(batch):\n",
    "    arr = []\n",
    "    for img in batch:\n",
    "        pil_img = transformations.ToPILImage()(img)\n",
    "        resized_img = pil_img.resize((299,299), Image.BILINEAR)\n",
    "        arr.append(transformations.ToTensor()(resized_img))\n",
    "    return torch.stack(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST FID \n",
    "\n",
    "# y_pred, y = torch.rand(100, 3, 64, 64), torch.rand(100, 3, 64, 64)\n",
    "# y_pred = interpolate(y_pred)\n",
    "# y = interpolate(y)\n",
    "# # m = FID()\n",
    "# fid_score.update((y_pred, y))\n",
    "\n",
    "# print('ignite batch FID', fid_score.compute())  # 8.98434072559458e-05\n",
    "# #reset the fid score\n",
    "# fid_score.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (disc): Sequential(\n",
       "    (0): Conv2d(4, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (5): Conv2d(128, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "  )\n",
       "  (embed): Embedding(10, 4096)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "gen.train()\n",
    "critic.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gradient penalty function for WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic,labels, real, fake, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, labels)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.91 GiB (GPU 1; 23.70 GiB total capacity; 16.70 GiB already allocated; 3.26 GiB free; 18.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/2_conditional_wgan_gp.ipynb Cell 36\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/2_conditional_wgan_gp.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(noise) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(labels):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/2_conditional_wgan_gp.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     noise \u001b[39m=\u001b[39m noise[:\u001b[39mlen\u001b[39m(labels)]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/2_conditional_wgan_gp.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m fake \u001b[39m=\u001b[39m gen(noise, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/2_conditional_wgan_gp.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m critic_real \u001b[39m=\u001b[39m critic(real, labels)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/2_conditional_wgan_gp.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m critic_fake \u001b[39m=\u001b[39m critic(fake, labels)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/2_conditional_wgan_gp.ipynb Cell 36\u001b[0m in \u001b[0;36mGenerator.forward\u001b[0;34m(self, x, labels)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/2_conditional_wgan_gp.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed(labels)\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m3\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/2_conditional_wgan_gp.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x, embedding], \u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/2_conditional_wgan_gp.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.91 GiB (GPU 1; 23.70 GiB total capacity; 16.70 GiB already allocated; 3.26 GiB free; 18.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    #we will track the total loss of the generator and critic for each epoch over the entire dataset\n",
    "    #initialize the total loss of the generator and critic for each epoch to 0\n",
    "    total_loss_gen = 0\n",
    "    total_loss_critic = 0\n",
    "    \n",
    "    #move these to device\n",
    "\n",
    "    #have no gradient for these losse\n",
    "    # total_loss_gen = torch.no_grad()\n",
    "    # total_loss_critic = torch.no_grad()\n",
    "    \n",
    "    # Target labels \n",
    "    for batch_idx, (real, labels) in enumerate(loader):\n",
    "        #send labels to device\n",
    "        labels = labels.to(device)\n",
    "        batch_step = 0\n",
    "        real = real.to(device)\n",
    "        cur_batch_size = real.shape[0]\n",
    "        \n",
    "        \n",
    "\n",
    "        # Train Critic: \n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
    "            if len(noise) != len(labels):\n",
    "                noise = noise[:len(labels)]\n",
    "            fake = gen(noise, labels)\n",
    "            critic_real = critic(real, labels).reshape(-1)\n",
    "            critic_fake = critic(fake, labels).reshape(-1)\n",
    "            gp = gradient_penalty(critic, real, fake, device=device)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
    "            )\n",
    "            \n",
    "            \n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step()\n",
    "            \n",
    "        #trained critic\n",
    "        \n",
    "\n",
    "        # Train Generator: \n",
    "        gen_fake = critic(fake, labels).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #update the total loss of the generator and critic for each batch in the epoch\n",
    "        #add just the value no gradients\n",
    "        #have no gradient for these losse\n",
    "        #add just the value no gradientsfrom the loss_gen tensor\n",
    "      \n",
    "        with torch.no_grad():\n",
    "            total_loss_gen += loss_gen.item()\n",
    "            total_loss_critic += loss_critic.item()\n",
    "        \n",
    "        # Print losses occasionally and print to tensorboard in a batch\n",
    "        if batch_idx % 10 == 0 and batch_idx > 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} \\\n",
    "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                #BATCH LOSS-----\n",
    "                \n",
    "                #write gen_loss and critic_loss to tensorboard\n",
    "                writer_loss.add_scalar(\"Generator loss Batch\", loss_gen, global_step=step)\n",
    "                writer_loss.add_scalar(\"Critic loss Batch\", loss_critic, global_step=step)\n",
    "                \n",
    "                #BATCH LOSS-------\n",
    "\n",
    "                \n",
    "                #FID--\n",
    "                #calculate FID score of this batch\n",
    "                #update the fid_score with real and fake images\n",
    "                real_images_fid = interpolate(real)\n",
    "                fake_images_fid = interpolate(fake)\n",
    "                fid_score.update((real_images_fid, fake_images_fid))\n",
    "                computed_fid_score = fid_score.compute()\n",
    "                print(\"FID score: \", computed_fid_score)\n",
    "                writer_loss.add_scalar(\"FID Score WGAN\", computed_fid_score, global_step=batch_step)\n",
    "                #reset the fid score\n",
    "                fid_score.reset()\n",
    "                ##FID--\n",
    "                \n",
    "                batch_step += 1\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "        # Print losses occasionally and print to tensorboard per epoch\n",
    "        # if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "        \n",
    "            ## PRINT for few epochs %10\n",
    "            # print(\n",
    "            #     f\"Epoch [{epoch}/{NUM_EPOCHS}]  \\\n",
    "            #         Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            # )\n",
    "            \n",
    "    #calculate the Fréchet Inception Distance (FID) to evaluate the performance of the generator\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake = gen(fixed_noise, fixed_labels)\n",
    "        # take out (up to) 32 examples\n",
    "        img_grid_real = torchvision.utils.make_grid(real[:100], normalize=True)\n",
    "        img_grid_fake = torchvision.utils.make_grid(fake[:100], normalize=True)\n",
    "\n",
    "        writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "        writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "        \n",
    "        \n",
    "                        \n",
    "        #AVERAGE LOSS---\n",
    "\n",
    "        #get average loss of generator and critic for each epoch\n",
    "        avg_loss_gen = total_loss_gen / len(loader)\n",
    "        avg_loss_critic = total_loss_critic / len(loader)\n",
    "        #write loss to tensorboard\n",
    "        writer_loss.add_scalar(\"Generator loss Epoch\", avg_loss_gen, global_step=batch_step)\n",
    "        writer_loss.add_scalar(\"Critic loss Epoch\", avg_loss_critic, global_step=batch_step)\n",
    "        \n",
    "        #AVERAGE LOSS----\n",
    "        \n",
    "        \n",
    "        \n",
    "        #we will plot the gradient of critic output with respect to the input image\n",
    "        #get the gradient of the critic output with respect to the input image\n",
    "        gradient = torch.autograd.grad(\n",
    "        inputs=real,\n",
    "        outputs=critic_real,\n",
    "        grad_outputs=torch.ones_like(critic_real),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        )[0]\n",
    "        #flatten the gradient\n",
    "        gradient = gradient.view(gradient.shape[0], -1)\n",
    "        #get the norm of the gradient\n",
    "        gradient_norm = gradient.norm(2, dim=1)\n",
    "        #write gradient norm to tensorboard\n",
    "        writer_loss.add_scalar(\"Gradient norm Critic Real\", gradient_norm.mean(), global_step=step)\n",
    "        \n",
    "        #----------------\n",
    "        #we will plot the gradient of critic output with respect to the input image\n",
    "        #get the gradient of the critic output with respect to the input image\n",
    "        gradient = torch.autograd.grad(\n",
    "        inputs=fake,\n",
    "        outputs=critic_fake,\n",
    "        grad_outputs=torch.ones_like(critic_fake),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        )[0]\n",
    "        #flatten the gradient\n",
    "        gradient = gradient.view(gradient.shape[0], -1)\n",
    "        #get the norm of the gradient\n",
    "        gradient_norm = gradient.norm(2, dim=1)\n",
    "        #write gradient norm to tensorboard\n",
    "        writer_loss.add_scalar(\"Gradient norm Critic Fake\", gradient_norm.mean(), global_step=step)\n",
    "        \n",
    "        #----------------\n",
    "        #we will plot the gradient of genrator output with respect to the input \n",
    "        #we will plot the gradient of genrator output with respect to the input \n",
    "        #get the gradient of the generator output with respect to the input noise\n",
    "        gradient = torch.autograd.grad(\n",
    "        inputs=noise,\n",
    "        outputs=gen_fake,\n",
    "        grad_outputs=torch.ones_like(gen_fake),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        )[0]\n",
    "        #flatten the gradient\n",
    "        gradient = gradient.view(gradient.shape[0], -1)\n",
    "        #get the norm of the gradient\n",
    "        gradient_norm = gradient.norm(2, dim=1)\n",
    "        #write gradient norm to tensorboard\n",
    "        writer_loss.add_scalar(\"Gradient norm Generator\", gradient_norm.mean(), global_step=step)\n",
    "        \n",
    "        #----------------\n",
    "\n",
    "        \n",
    "        \n",
    "        # we will plot the gradient penalty\n",
    "        writer_loss.add_scalar(\"GP\", gp, global_step=step)\n",
    "        #we will analyze for vanishing gradient\n",
    "        writer_loss.add_scalar(\"Critic Real\", critic_real.mean(), global_step=step)\n",
    "        writer_loss.add_scalar(\"Critic Fake\", critic_fake.mean(), global_step=step)\n",
    "        \n",
    "        #get the gradient of the critic for the parameters weights of first layer\n",
    "        #we will write the norm of the gardient of weights of the first layer of the critic\n",
    "        for name, param in critic.named_parameters():\n",
    "            if name == \"disc.0.weight\":\n",
    "                writer_loss.add_scalar(\"Critic Gradient w.r.t 1st layer WGAN\", param.grad.norm(), global_step=step)\n",
    "            #also plot the norm of gradient of 2nd layer\n",
    "            elif name == \"disc.2.0.weight\":\n",
    "                writer_loss.add_scalar(\"Critic Gradient w.r.t 2nd layer WGAN\", param.grad.norm(), global_step=step)\n",
    "        \n",
    "       \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    step += 1\n",
    "        \n",
    "        #save the trained model\n",
    "        #check if trained_model folder exists\n",
    "    if not os.path.exists(\"trained_models\"):\n",
    "        os.mkdir(\"trained_models\")\n",
    "    \n",
    "    #now trained_model folder exists\n",
    "    if not os.path.exists(\"trained_models/\"+model_name):\n",
    "        os.mkdir(\"trained_models/\"+model_name)\n",
    "    #check if \"trained_models/\"+model_name     \n",
    "    torch.save(gen.state_dict(), \"trained_models/\"+model_name+\"/gen.pth\")\n",
    "    torch.save(critic.state_dict(), \"trained_models/\"+model_name+\"/critic.pth\")\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the tensorboard\n",
    "writer_real.close()\n",
    "writer_fake.close()\n",
    "writer_loss.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the Fréchet Inception Distance (FID) to evaluate the performance of the generator\n",
    "\n",
    "#(https://github.com/mseitzer/pytorch-fid)\n",
    "#imports for FID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/2_conditional_wgan_gp.ipynb Cell 39\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/2_conditional_wgan_gp.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#print generator model architecture\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/2_conditional_wgan_gp.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(gen)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen' is not defined"
     ]
    }
   ],
   "source": [
    "#print generator model architecture\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "079402cc50f681fca3bc4b588c8594ae5b0127c6215ec7c89d21fdfb87f97274"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
