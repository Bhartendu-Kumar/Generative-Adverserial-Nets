{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will implement DCGAN on SVHN dataset. The dataset is available at http://ufldl.stanford.edu/housenumbers/. The dataset is a collection of 32x32 color images of house numbers. The dataset is split into 3 parts: train, test and extra. We will use the train and test set for training and testing respectively. The extra set is not used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dcgan1\"\n",
    "#check model saving path is there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms as transformations\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "import torchvision.models as models\n",
    "#numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from ignite.metrics.gan import FID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and constants: for dataset and training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters etc.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 2e-4  # could also use two lrs, one for gen and one for disc\n",
    "BATCH_SIZE = 3200\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS_IMG = 3\n",
    "NOISE_DIM = 100\n",
    "NUM_EPOCHS = 5\n",
    "FEATURES_DISC = 64\n",
    "FEATURES_GEN = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we will use SVHN dataset for this example\n",
    "### we will combine the train, test and extra datasets to make a bigger dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: dataset_svhm/train_32x32.mat\n",
      "Using downloaded and verified file: dataset_svhm/test_32x32.mat\n",
      "Using downloaded and verified file: dataset_svhm/extra_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "#get the dataset\n",
    "#train part of svhn\n",
    "train_dataset = datasets.SVHN(root=\"dataset_svhm/\", split='train', transform=transforms, download=True)\n",
    "#test part of svhn\n",
    "test_dataset = datasets.SVHN(root=\"dataset_svhm/\", split='test', transform=transforms, download=True)\n",
    "#extra part of svhn\n",
    "extra_dataset = datasets.SVHN(root=\"dataset_svhm/\", split='extra', transform=transforms, download=True)\n",
    "#concatenate the train, test and extra dataset\n",
    "dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset, extra_dataset])\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630420\n",
      "torch.Size([3, 64, 64])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#print the total number of images in the dataset\n",
    "print(len(dataset))\n",
    "# print the shape of the images\n",
    "print(dataset[0][0].shape)\n",
    "# print the label of the image\n",
    "print(dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "           \n",
    "            self.generator_block_architecture(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
    "            self.generator_block_architecture(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
    "            self.generator_block_architecture(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
    "            self.generator_block_architecture(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "     \n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def generator_block_architecture(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(\n",
    "                channels_img, features_d, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self.dicriminator_block_architecture(features_d, features_d * 2, 4, 2, 1),\n",
    "            self.dicriminator_block_architecture(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self.dicriminator_block_architecture(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "       \n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def dicriminator_block_architecture(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization : Model , Loss , Optimizer, data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(disc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-03 12:41:05.533295: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fixed_noise = torch.randn(100, NOISE_DIM, 1, 1).to(device)\n",
    "#plot loss of generator and critic\n",
    "writer_loss = SummaryWriter(f\"runs/\"+model_name+\"/loss\")\n",
    "writer_real = SummaryWriter(f\"logs/\"+model_name+\"/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/\"+model_name+\"/fake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize FID wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_score = FID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpolate function to resize images to 299,299,3  which is the input size of inception network\n",
    "def interpolate(batch):\n",
    "    arr = []\n",
    "    for img in batch:\n",
    "        pil_img = transformations.ToPILImage()(img)\n",
    "        resized_img = pil_img.resize((299,299), Image.BILINEAR)\n",
    "        arr.append(transformations.ToTensor()(resized_img))\n",
    "    return torch.stack(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (disc): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.train()\n",
    "disc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5] Batch 0/198                   Loss D: 0.6551, loss G: 0.8385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3633/595671961.py:6: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  resized_img = pil_img.resize((299,299), Image.BILINEAR)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID score:  0.15315825285524987\n",
      "Epoch [0/5] Batch 10/198                   Loss D: 0.3857, loss G: 1.3355\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/dcgan.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/dcgan.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m real_images_fid \u001b[39m=\u001b[39m interpolate(real)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/dcgan.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m fake_images_fid \u001b[39m=\u001b[39m interpolate(fake)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/dcgan.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m fid_score\u001b[39m.\u001b[39;49mupdate((real_images_fid, fake_images_fid))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/dcgan.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m computed_fid_score \u001b[39m=\u001b[39m fid_score\u001b[39m.\u001b[39mcompute()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.35.99/home/lisa/bhartendu/adrl/Assignment1/conditional_gan/dcgan.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFID score: \u001b[39m\u001b[39m\"\u001b[39m, computed_fid_score)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/ignite/metrics/metric.py:596\u001b[0m, in \u001b[0;36mreinit__is_reduced.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    595\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39mself\u001b[39m: Metric, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_reduced \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/ignite/metrics/gan/fid.py:236\u001b[0m, in \u001b[0;36mFID.update\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m@reinit__is_reduced\u001b[39m\n\u001b[1;32m    233\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\u001b[39mself\u001b[39m, output: Sequence[torch\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     train, test \u001b[39m=\u001b[39m output\n\u001b[0;32m--> 236\u001b[0m     train_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_features(train)\n\u001b[1;32m    237\u001b[0m     test_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extract_features(test)\n\u001b[1;32m    239\u001b[0m     \u001b[39mif\u001b[39;00m train_features\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m test_features\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39mor\u001b[39;00m train_features\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m test_features\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/ignite/metrics/gan/utils.py:101\u001b[0m, in \u001b[0;36m_BaseInceptionMetric._extract_features\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     98\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device)\n\u001b[1;32m    100\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 101\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feature_extractor(inputs)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m    102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_feature_shapes(outputs)\n\u001b[1;32m    104\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/ignite/metrics/gan/utils.py:49\u001b[0m, in \u001b[0;36mInceptionModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mdevice \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device):\n\u001b[1;32m     48\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device)\n\u001b[0;32m---> 49\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/inception.py:166\u001b[0m, in \u001b[0;36mInception3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InceptionOutputs:\n\u001b[1;32m    165\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform_input(x)\n\u001b[0;32m--> 166\u001b[0m     x, aux \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(x)\n\u001b[1;32m    167\u001b[0m     aux_defined \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maux_logits\n\u001b[1;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/inception.py:123\u001b[0m, in \u001b[0;36mInception3._forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    121\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMixed_5c(x)\n\u001b[1;32m    122\u001b[0m \u001b[39m# N x 288 x 35 x 35\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMixed_5d(x)\n\u001b[1;32m    124\u001b[0m \u001b[39m# N x 288 x 35 x 35\u001b[39;00m\n\u001b[1;32m    125\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMixed_6a(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/inception.py:211\u001b[0m, in \u001b[0;36mInceptionA.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 211\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(x)\n\u001b[1;32m    212\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(outputs, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/inception.py:198\u001b[0m, in \u001b[0;36mInceptionA._forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    195\u001b[0m branch1x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbranch1x1(x)\n\u001b[1;32m    197\u001b[0m branch5x5 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbranch5x5_1(x)\n\u001b[0;32m--> 198\u001b[0m branch5x5 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbranch5x5_2(branch5x5)\n\u001b[1;32m    200\u001b[0m branch3x3dbl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbranch3x3dbl_1(x)\n\u001b[1;32m    201\u001b[0m branch3x3dbl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbranch3x3dbl_2(branch3x3dbl)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/inception.py:407\u001b[0m, in \u001b[0;36mBasicConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    405\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv(x)\n\u001b[1;32m    406\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(x)\n\u001b[0;32m--> 407\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mrelu(x, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    \n",
    "    #we will track the total loss of the generator and critic for each epoch over the entire dataset\n",
    "    #initialize the total loss of the generator and critic for each epoch to 0\n",
    "    total_loss_gen = 0\n",
    "    total_loss_disc = 0\n",
    "    #move these to device\n",
    "    \n",
    "    \n",
    "    # Target labels not needed! <3 unsupervised\n",
    "    for batch_idx, (real, _) in enumerate(dataloader):\n",
    "        batch_step = 0\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
    "        fake = gen(noise)\n",
    "\n",
    "        ### Train Discriminator\n",
    "        disc_real = disc(real).reshape(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake.detach()).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator:\n",
    "        output = disc(fake).reshape(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            total_loss_gen += loss_gen.item()\n",
    "            total_loss_disc += loss_disc.item()\n",
    "            \n",
    "        \n",
    "\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                               \n",
    "                 #BATCH LOSS---\n",
    "        \n",
    "                #write loss to tensorboard\n",
    "                writer_loss.add_scalar(\"Generator loss Batch\", loss_gen, global_step=batch_step)\n",
    "                writer_loss.add_scalar(\"Discriminator loss Batch\", loss_disc, global_step=batch_step)         \n",
    "                \n",
    "                #FID--\n",
    "                #calculate FID score of this batch\n",
    "                #update the fid_score with real and fake images\n",
    "                real_images_fid = interpolate(real)\n",
    "                fake_images_fid = interpolate(fake)\n",
    "                fid_score.update((real_images_fid, fake_images_fid))\n",
    "                computed_fid_score = fid_score.compute()\n",
    "                print(\"FID score: \", computed_fid_score)\n",
    "                writer_loss.add_scalar(\"FID Score DCGAN\", computed_fid_score, global_step=batch_step)\n",
    "                #reset the fid score\n",
    "                fid_score.reset()\n",
    "                ##FID--\n",
    "                \n",
    "                batch_step += 1 \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake = gen(fixed_noise)\n",
    "        # take out upto 100 examples\n",
    "        img_grid_real = torchvision.utils.make_grid(\n",
    "            real[:100], normalize=True\n",
    "        )\n",
    "        img_grid_fake = torchvision.utils.make_grid(\n",
    "            fake[:100], normalize=True\n",
    "        )\n",
    "\n",
    "        writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "        writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "        \n",
    "        \n",
    "        #AVERAGE LOSS---\n",
    "\n",
    "        #get average loss of generator and critic for each epoch\n",
    "        avg_loss_gen = total_loss_gen / len(loader)\n",
    "        avg_loss_disc= total_loss_disc / len(loader)\n",
    "        #write loss to tensorboard\n",
    "        writer_loss.add_scalar(\"Generator loss Epoch\", avg_loss_gen, global_step=batch_step)\n",
    "        writer_loss.add_scalar(\"Discriminator loss Epoch\", avg_loss_disc, global_step=batch_step)\n",
    "        \n",
    "        #AVERAGE LOSS----\n",
    "        \n",
    "        #we will plot the gradient of disc output with respect to the input image\n",
    "        #get the gradient of the disc output with respect to the input image\n",
    "        gradient = torch.autograd.grad(\n",
    "        inputs=real,\n",
    "        outputs=disc_real,\n",
    "        grad_outputs=torch.ones_like(disc_real),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        )[0]\n",
    "        #flatten the gradient\n",
    "        gradient = gradient.view(gradient.shape[0], -1)\n",
    "        #get the norm of the gradient\n",
    "        gradient_norm = gradient.norm(2, dim=1)\n",
    "        #write gradient norm to tensorboard\n",
    "        writer_loss.add_scalar(\"Gradient norm Disc Real DCGAN\", gradient_norm.mean(), global_step=step)\n",
    "        \n",
    "        #----------------\n",
    "        #we will plot the gradient of critic output with respect to the input image\n",
    "        #get the gradient of the critic output with respect to the input image\n",
    "        gradient = torch.autograd.grad(\n",
    "        inputs=fake,\n",
    "        outputs=disc_fake,\n",
    "        grad_outputs=torch.ones_like(disc_fake),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        )[0]\n",
    "        #flatten the gradient\n",
    "        gradient = gradient.view(gradient.shape[0], -1)\n",
    "        #get the norm of the gradient\n",
    "        gradient_norm = gradient.norm(2, dim=1)\n",
    "        #write gradient norm to tensorboard\n",
    "        writer_loss.add_scalar(\"Gradient norm Disc Fake DCGAN\", gradient_norm.mean(), global_step=step)\n",
    "        \n",
    "        #----------------\n",
    "        #we will plot the gradient of genrator output with respect to the input \n",
    "        #we will plot the gradient of genrator output with respect to the input \n",
    "        #get the gradient of the generator output with respect to the input noise\n",
    "        gradient = torch.autograd.grad(\n",
    "        inputs=noise,\n",
    "        outputs=output,\n",
    "        grad_outputs=torch.ones_like(output),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        )[0]\n",
    "        #flatten the gradient\n",
    "        gradient = gradient.view(gradient.shape[0], -1)\n",
    "        #get the norm of the gradient\n",
    "        gradient_norm = gradient.norm(2, dim=1)\n",
    "        #write gradient norm to tensorboard\n",
    "        writer_loss.add_scalar(\"Gradient norm Generator DCGAN\", gradient_norm.mean(), global_step=step)\n",
    "        \n",
    "        #----------------\n",
    "        \n",
    "        #get the gradient of the disc for the parameters weights of first layer\n",
    "        #we will write the norm of the gardient of weights of the first layer of the disc\n",
    "        for name, param in critic.named_parameters():\n",
    "            if name == \"disc.0.weight\":\n",
    "                writer_loss.add_scalar(\"Disc Gradient w.r.t 1st layer DCGAN\", param.grad.norm(), global_step=step)\n",
    "            #also plot the norm of gradient of 2nd layer\n",
    "            elif name == \"disc.2.0.weight\":\n",
    "                writer_loss.add_scalar(\"Disc Gradient w.r.t 2nd layer DCGAN\", param.grad.norm(), global_step=step)\n",
    "                \n",
    "                \n",
    "       \n",
    "\n",
    "    step += 1\n",
    "    \n",
    "    #save the trained model\n",
    "        #check if trained_model folder exists\n",
    "    if not os.path.exists(\"trained_models\"):\n",
    "        os.mkdir(\"trained_models\")\n",
    "    \n",
    "    #now trained_model folder exists\n",
    "    if not os.path.exists(\"trained_models/\"+model_name):\n",
    "        os.mkdir(\"trained_models/\"+model_name)\n",
    "    #check if \"trained_models/\"+model_name     \n",
    "    torch.save(gen.state_dict(), \"trained_models/\"+model_name+\"/gen.pth\")\n",
    "    torch.save(critic.state_dict(), \"trained_models/\"+model_name+\"/disc.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the tensorboard\n",
    "writer_real.close()\n",
    "writer_fake.close()\n",
    "writer_loss.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "079402cc50f681fca3bc4b588c8594ae5b0127c6215ec7c89d21fdfb87f97274"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
